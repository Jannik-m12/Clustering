{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715b3704",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing for Clustering\n",
    "\n",
    "## Overview\n",
    "This notebook prepares the data for clustering analysis by handling missing values, feature selection, scaling, and dimensionality reduction.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete `01_Data_Exploration_EDA.ipynb` first\n",
    "- Dataset: `../Data/eda_complete_dataset.csv`\n",
    "\n",
    "**Objectives:**\n",
    "- Feature engineering and selection\n",
    "- Handle outliers and missing values\n",
    "- Scale and normalize features\n",
    "- Apply dimensionality reduction techniques\n",
    "- Prepare data for clustering algorithms\n",
    "\n",
    "**Outputs:**\n",
    "- Preprocessed dataset ready for clustering\n",
    "- Scaled features\n",
    "- PCA and t-SNE transformed data\n",
    "- Feature importance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bfde9a",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6dcdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Preprocessing and Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer, LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Outlier Detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Preprocessing libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12478e3",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "663827f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: (11000, 16)\n",
      "Columns: ['CompanyID', 'CompanyName', 'Industry', 'Region', 'Year', 'Revenue', 'ProfitMargin', 'MarketCap', 'GrowthRate', 'ESG_Overall', 'ESG_Environmental', 'ESG_Social', 'ESG_Governance', 'CarbonEmissions', 'WaterUsage', 'EnergyConsumption']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Region</th>\n",
       "      <th>Year</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>ProfitMargin</th>\n",
       "      <th>MarketCap</th>\n",
       "      <th>GrowthRate</th>\n",
       "      <th>ESG_Overall</th>\n",
       "      <th>ESG_Environmental</th>\n",
       "      <th>ESG_Social</th>\n",
       "      <th>ESG_Governance</th>\n",
       "      <th>CarbonEmissions</th>\n",
       "      <th>WaterUsage</th>\n",
       "      <th>EnergyConsumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2015</td>\n",
       "      <td>459.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>337.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.7</td>\n",
       "      <td>33.5</td>\n",
       "      <td>76.8</td>\n",
       "      <td>35577.4</td>\n",
       "      <td>17788.7</td>\n",
       "      <td>71154.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2016</td>\n",
       "      <td>473.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>366.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>56.7</td>\n",
       "      <td>58.9</td>\n",
       "      <td>32.8</td>\n",
       "      <td>78.5</td>\n",
       "      <td>37314.7</td>\n",
       "      <td>18657.4</td>\n",
       "      <td>74629.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2017</td>\n",
       "      <td>564.9</td>\n",
       "      <td>5.2</td>\n",
       "      <td>313.4</td>\n",
       "      <td>19.2</td>\n",
       "      <td>56.5</td>\n",
       "      <td>57.6</td>\n",
       "      <td>34.0</td>\n",
       "      <td>77.8</td>\n",
       "      <td>45006.4</td>\n",
       "      <td>22503.2</td>\n",
       "      <td>90012.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2018</td>\n",
       "      <td>558.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>283.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>62.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>78.3</td>\n",
       "      <td>42650.1</td>\n",
       "      <td>21325.1</td>\n",
       "      <td>85300.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2019</td>\n",
       "      <td>554.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>538.1</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>56.6</td>\n",
       "      <td>63.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>76.1</td>\n",
       "      <td>41799.4</td>\n",
       "      <td>20899.7</td>\n",
       "      <td>83598.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CompanyID CompanyName Industry         Region  Year  Revenue  ProfitMargin  \\\n",
       "0          1   Company_1   Retail  Latin America  2015    459.2           6.0   \n",
       "1          1   Company_1   Retail  Latin America  2016    473.8           4.6   \n",
       "2          1   Company_1   Retail  Latin America  2017    564.9           5.2   \n",
       "3          1   Company_1   Retail  Latin America  2018    558.4           4.3   \n",
       "4          1   Company_1   Retail  Latin America  2019    554.5           4.9   \n",
       "\n",
       "   MarketCap  GrowthRate  ESG_Overall  ESG_Environmental  ESG_Social  \\\n",
       "0      337.5         NaN         57.0               60.7        33.5   \n",
       "1      366.6         3.2         56.7               58.9        32.8   \n",
       "2      313.4        19.2         56.5               57.6        34.0   \n",
       "3      283.0        -1.1         58.0               62.3        33.4   \n",
       "4      538.1        -0.7         56.6               63.7        30.0   \n",
       "\n",
       "   ESG_Governance  CarbonEmissions  WaterUsage  EnergyConsumption  \n",
       "0            76.8          35577.4     17788.7            71154.7  \n",
       "1            78.5          37314.7     18657.4            74629.4  \n",
       "2            77.8          45006.4     22503.2            90012.9  \n",
       "3            78.3          42650.1     21325.1            85300.2  \n",
       "4            76.1          41799.4     20899.7            83598.8  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from EDA notebook\n",
    "try:\n",
    "    data = pd.read_csv('../Data/eda_complete_dataset.csv')\n",
    "    print(f\"Dataset loaded successfully: {data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"EDA dataset not found. Please run 01_Data_Exploration_EDA.ipynb first.\")\n",
    "    # Fallback to original dataset\n",
    "    data = pd.read_csv('../Data/company_esg_financial_dataset.csv')\n",
    "    print(f\"Using original dataset: {data.shape}\")\n",
    "\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5824201",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0051d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial features: ['Revenue', 'ProfitMargin', 'MarketCap', 'GrowthRate']\n",
      "ESG features: ['ESG_Overall', 'ESG_Environmental', 'ESG_Social', 'ESG_Governance']\n",
      "Environmental features: ['CarbonEmissions', 'WaterUsage', 'EnergyConsumption']\n",
      "Categorical features: ['Industry', 'Region']\n",
      "\n",
      "Total numerical features for clustering: 11\n"
     ]
    }
   ],
   "source": [
    "# Define feature categories\n",
    "financial_features = ['Revenue', 'ProfitMargin', 'MarketCap', 'GrowthRate']\n",
    "esg_features = ['ESG_Overall', 'ESG_Environmental', 'ESG_Social', 'ESG_Governance']\n",
    "environmental_features = ['CarbonEmissions', 'WaterUsage', 'EnergyConsumption']\n",
    "categorical_features = ['Industry', 'Region']\n",
    "identifier_features = ['CompanyID', 'CompanyName', 'Year']\n",
    "\n",
    "# Combine numerical features for clustering\n",
    "numerical_features = financial_features + esg_features + environmental_features\n",
    "\n",
    "print(f\"Financial features: {financial_features}\")\n",
    "print(f\"ESG features: {esg_features}\")\n",
    "print(f\"Environmental features: {environmental_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"\\nTotal numerical features for clustering: {len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a617042",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values and Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8af2afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in numerical features:\n",
      "GrowthRate    1000\n",
      "dtype: int64\n",
      "Filled GrowthRate missing values with median: 4.90\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in numerical features\n",
    "missing_check = data[numerical_features].isnull().sum()\n",
    "print(\"Missing values in numerical features:\")\n",
    "print(missing_check[missing_check > 0])\n",
    "\n",
    "# Handle missing values if any\n",
    "if missing_check.sum() > 0:\n",
    "    # Fill missing values with median for numerical features\n",
    "    for feature in numerical_features:\n",
    "        if data[feature].isnull().sum() > 0:\n",
    "            median_value = data[feature].median()\n",
    "            data[feature].fillna(median_value, inplace=True)\n",
    "            print(f\"Filled {feature} missing values with median: {median_value:.2f}\")\n",
    "else:\n",
    "    print(\"No missing values found in numerical features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78070fb",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81bcb9",
   "metadata": {},
   "source": [
    "### 4.1 Advanced Transformation for Skewed Features\n",
    "\n",
    "Based on EDA analysis, 5 features show extreme skewness (|skew| >= 1.0):\n",
    "- CarbonEmissions: 15.848\n",
    "- EnergyConsumption: 15.654  \n",
    "- WaterUsage: 14.386\n",
    "- MarketCap: 8.884\n",
    "- Revenue: 7.369\n",
    "\n",
    "Standard scaling alone doesn't fix skewness. We'll apply power transformation first, then scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8d9e9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING PIPELINE WITH SKEWNESS CORRECTION\n",
      "============================================================\n",
      "\n",
      "1. ANALYZING ORIGINAL SKEWNESS:\n",
      "   • Revenue: 7.369 (HIGH SKEWNESS)\n",
      "   • MarketCap: 8.884 (HIGH SKEWNESS)\n",
      "   • CarbonEmissions: 15.848 (HIGH SKEWNESS)\n",
      "   • WaterUsage: 14.386 (HIGH SKEWNESS)\n",
      "   • EnergyConsumption: 15.654 (HIGH SKEWNESS)\n",
      "\n",
      "2. APPLYING YEO-JOHNSON POWER TRANSFORMATION:\n",
      "   Post-transformation skewness:\n",
      "   • Revenue: 0.008 (improved by 7.361)\n",
      "   • MarketCap: -0.001 (improved by 8.883)\n",
      "   • CarbonEmissions: -0.014 (improved by 15.834)\n",
      "   • WaterUsage: -0.001 (improved by 14.385)\n",
      "   • EnergyConsumption: 0.019 (improved by 15.634)\n",
      "\n",
      "3. APPLYING SCALING TO POWER-TRANSFORMED DATA:\n",
      "   StandardScaler applied to power-transformed data.\n",
      "   MinMaxScaler applied to power-transformed data.\n",
      "   RobustScaler applied to power-transformed data.\n",
      "\n",
      "4. FINAL PREPROCESSING SUMMARY:\n",
      "   ✅ Power transformation applied to fix skewness\n",
      "   ✅ Standard scaling applied for clustering\n",
      "   📊 Final preprocessed data shape: (11000, 11)\n",
      "\n",
      "5. FINAL SKEWNESS VERIFICATION:\n",
      "   ✅ All features now have acceptable skewness (|skew| < 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for scaling\n",
    "X = data[numerical_features].copy()\n",
    "\n",
    "print(\"PREPROCESSING PIPELINE WITH SKEWNESS CORRECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Apply Power Transformation to handle skewness\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats import skew\n",
    "\n",
    "print(\"\\n1. ANALYZING ORIGINAL SKEWNESS:\")\n",
    "original_skew = {}\n",
    "for feature in numerical_features:\n",
    "    skewness = skew(X[feature].dropna())\n",
    "    original_skew[feature] = skewness\n",
    "    if abs(skewness) >= 1.0:\n",
    "        print(f\"   • {feature}: {skewness:.3f} (HIGH SKEWNESS)\")\n",
    "\n",
    "# Apply Yeo-Johnson Power Transformation (handles positive and negative values)\n",
    "print(\"\\n2. APPLYING YEO-JOHNSON POWER TRANSFORMATION:\")\n",
    "power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "X_power_transformed = power_transformer.fit_transform(X)\n",
    "X_power_df = pd.DataFrame(X_power_transformed, columns=numerical_features, index=X.index)\n",
    "\n",
    "# Check skewness after power transformation\n",
    "print(\"   Post-transformation skewness:\")\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    new_skewness = skew(X_power_transformed[:, i])\n",
    "    improvement = abs(original_skew[feature]) - abs(new_skewness)\n",
    "    if abs(original_skew[feature]) >= 1.0:\n",
    "        print(f\"   • {feature}: {new_skewness:.3f} (improved by {improvement:.3f})\")\n",
    "\n",
    "# Step 2: Apply different scaling methods to power-transformed data\n",
    "print(\"\\n3. APPLYING SCALING TO POWER-TRANSFORMED DATA:\")\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "scaled_data = {}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    scaled_features = scaler.fit_transform(X_power_df)\n",
    "    scaled_data[name] = pd.DataFrame(scaled_features, columns=numerical_features, index=X.index)\n",
    "    print(f\"   {name} applied to power-transformed data.\")\n",
    "\n",
    "# Use StandardScaler on power-transformed data as default\n",
    "X_scaled = scaled_data['StandardScaler']\n",
    "\n",
    "print(f\"\\n4. FINAL PREPROCESSING SUMMARY:\")\n",
    "print(f\"   ✅ Power transformation applied to fix skewness\")\n",
    "print(f\"   ✅ Standard scaling applied for clustering\")\n",
    "print(f\"   📊 Final preprocessed data shape: {X_scaled.shape}\")\n",
    "\n",
    "# Verify final skewness\n",
    "print(f\"\\n5. FINAL SKEWNESS VERIFICATION:\")\n",
    "final_highly_skewed = 0\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    final_skewness = skew(X_scaled.iloc[:, i])\n",
    "    if abs(final_skewness) >= 1.0:\n",
    "        final_highly_skewed += 1\n",
    "        print(f\"   ⚠️  {feature}: {final_skewness:.3f} (still high)\")\n",
    "\n",
    "if final_highly_skewed == 0:\n",
    "    print(\"   ✅ All features now have acceptable skewness (|skew| < 1.0)\")\n",
    "else:\n",
    "    print(f\"   ⚠️  {final_highly_skewed} features still highly skewed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cac869",
   "metadata": {},
   "source": [
    "## 6. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97ccf94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. DIMENSIONALITY REDUCTION WITH PCA:\n",
      "   Number of components for 95% variance: 7\n",
      "   Explained variance by first 3 components: 0.727\n",
      "   ✅ PCA transformed data shape: (11000, 7)\n",
      "\n",
      "   PCA Components Explained Variance:\n",
      "   • PC1: 0.315 (31.5%)\n",
      "   • PC2: 0.241 (24.1%)\n",
      "   • PC3: 0.171 (17.1%)\n",
      "   • PC4: 0.087 (8.7%)\n",
      "   • PC5: 0.070 (7.0%)\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA to the properly transformed and scaled data\n",
    "print(\"\\n6. DIMENSIONALITY REDUCTION WITH PCA:\")\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "\n",
    "print(f\"   Number of components for 95% variance: {n_components_95}\")\n",
    "print(f\"   Explained variance by first 3 components: {cumsum_var[2]:.3f}\")\n",
    "\n",
    "# Create PCA dataframe with optimal components\n",
    "pca_optimal = PCA(n_components=n_components_95)\n",
    "X_pca_optimal = pca_optimal.fit_transform(X_scaled)\n",
    "\n",
    "pca_columns = [f'PCA_{i+1}' for i in range(n_components_95)]\n",
    "X_pca_df = pd.DataFrame(X_pca_optimal, columns=pca_columns, index=X_scaled.index)\n",
    "\n",
    "print(f\"   ✅ PCA transformed data shape: {X_pca_df.shape}\")\n",
    "\n",
    "# Show PCA component importance\n",
    "print(f\"\\n   PCA Components Explained Variance:\")\n",
    "for i in range(min(5, n_components_95)):\n",
    "    print(f\"   • PC{i+1}: {pca.explained_variance_ratio_[i]:.3f} ({pca.explained_variance_ratio_[i]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "869e5406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. T-SNE VISUALIZATION PREPARATION:\n",
      "   ✅ t-SNE transformed data shape: (11000, 2)\n",
      "   ✅ Ready for clustering visualization\n",
      "   ✅ t-SNE transformed data shape: (11000, 2)\n",
      "   ✅ Ready for clustering visualization\n"
     ]
    }
   ],
   "source": [
    "# Apply t-SNE for visualization (using first few PCA components)\n",
    "print(\"\\n7. T-SNE VISUALIZATION PREPARATION:\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_pca_optimal[:, :5])  # Use first 5 PCA components\n",
    "\n",
    "X_tsne_df = pd.DataFrame(X_tsne, columns=['t-SNE_1', 't-SNE_2'], index=X_scaled.index)\n",
    "\n",
    "print(f\"   ✅ t-SNE transformed data shape: {X_tsne_df.shape}\")\n",
    "print(f\"   ✅ Ready for clustering visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87843614",
   "metadata": {},
   "source": [
    "## 7. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f2fc079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. SAVING IMPROVED PREPROCESSED DATASETS:\n",
      "   ✅ Improved preprocessed datasets saved:\n",
      "   - ../Data/preprocessed_complete_dataset.csv\n",
      "   - ../Data/scaled_features.csv (power-transformed + scaled)\n",
      "   - ../Data/pca_features.csv (based on improved features)\n",
      "   - ../Data/tsne_features.csv\n",
      "   - ../Data/power_transformed_features.csv (intermediate step)\n",
      "\n",
      "🎯 PREPROCESSING COMPLETE - READY FOR IMPROVED CLUSTERING!\n",
      "=================================================================\n",
      "✅ Skewness corrected with Yeo-Johnson power transformation\n",
      "✅ Features properly scaled with StandardScaler\n",
      "✅ Dimensionality reduced with PCA (7 components)\n",
      "✅ Visualization prepared with t-SNE\n",
      "📊 Final dataset shapes:\n",
      "   • Scaled features: (11000, 11)\n",
      "   • PCA features: (11000, 7)\n",
      "   • t-SNE features: (11000, 2)\n",
      "   • Power transformer saved for future use\n",
      "   ✅ Improved preprocessed datasets saved:\n",
      "   - ../Data/preprocessed_complete_dataset.csv\n",
      "   - ../Data/scaled_features.csv (power-transformed + scaled)\n",
      "   - ../Data/pca_features.csv (based on improved features)\n",
      "   - ../Data/tsne_features.csv\n",
      "   - ../Data/power_transformed_features.csv (intermediate step)\n",
      "\n",
      "🎯 PREPROCESSING COMPLETE - READY FOR IMPROVED CLUSTERING!\n",
      "=================================================================\n",
      "✅ Skewness corrected with Yeo-Johnson power transformation\n",
      "✅ Features properly scaled with StandardScaler\n",
      "✅ Dimensionality reduced with PCA (7 components)\n",
      "✅ Visualization prepared with t-SNE\n",
      "📊 Final dataset shapes:\n",
      "   • Scaled features: (11000, 11)\n",
      "   • PCA features: (11000, 7)\n",
      "   • t-SNE features: (11000, 2)\n",
      "   • Power transformer saved for future use\n"
     ]
    }
   ],
   "source": [
    "# Combine all preprocessed data\n",
    "preprocessed_data = data.copy()\n",
    "\n",
    "# Add scaled features\n",
    "for col in X_scaled.columns:\n",
    "    preprocessed_data[f'{col}_scaled'] = X_scaled[col]\n",
    "\n",
    "# Add PCA features\n",
    "for col in X_pca_df.columns:\n",
    "    preprocessed_data[col] = X_pca_df[col]\n",
    "\n",
    "# Add t-SNE features\n",
    "for col in X_tsne_df.columns:\n",
    "    preprocessed_data[col] = X_tsne_df[col]\n",
    "\n",
    "# Save datasets with improved preprocessing\n",
    "print(\"\\n8. SAVING IMPROVED PREPROCESSED DATASETS:\")\n",
    "\n",
    "# Save power-transformed features (before scaling) for reference\n",
    "X_power_df.to_csv('../Data/power_transformed_features.csv', index=False)\n",
    "\n",
    "# Update the complete preprocessed dataset\n",
    "preprocessed_data = data.copy()\n",
    "\n",
    "# Add power-transformed + scaled features (our improved approach)\n",
    "for col in X_scaled.columns:\n",
    "    preprocessed_data[f'{col}_scaled'] = X_scaled[col]\n",
    "\n",
    "# Add PCA features (based on improved preprocessing)\n",
    "for col in X_pca_df.columns:\n",
    "    preprocessed_data[col] = X_pca_df[col]\n",
    "\n",
    "# Add t-SNE features\n",
    "for col in X_tsne_df.columns:\n",
    "    preprocessed_data[col] = X_tsne_df[col]\n",
    "\n",
    "# Save all datasets\n",
    "preprocessed_data.to_csv('../Data/preprocessed_complete_dataset.csv', index=False)\n",
    "X_scaled.to_csv('../Data/scaled_features.csv', index=False)  # Now power-transformed + scaled\n",
    "X_pca_df.to_csv('../Data/pca_features.csv', index=False)\n",
    "X_tsne_df.to_csv('../Data/tsne_features.csv', index=False)\n",
    "\n",
    "print(\"   ✅ Improved preprocessed datasets saved:\")\n",
    "print(\"   - ../Data/preprocessed_complete_dataset.csv\")\n",
    "print(\"   - ../Data/scaled_features.csv (power-transformed + scaled)\")\n",
    "print(\"   - ../Data/pca_features.csv (based on improved features)\")\n",
    "print(\"   - ../Data/tsne_features.csv\")\n",
    "print(\"   - ../Data/power_transformed_features.csv (intermediate step)\")\n",
    "\n",
    "print(f\"\\n🎯 PREPROCESSING COMPLETE - READY FOR IMPROVED CLUSTERING!\")\n",
    "print(\"=\"*65)\n",
    "print(f\"✅ Skewness corrected with Yeo-Johnson power transformation\")\n",
    "print(f\"✅ Features properly scaled with StandardScaler\")\n",
    "print(f\"✅ Dimensionality reduced with PCA ({n_components_95} components)\")\n",
    "print(f\"✅ Visualization prepared with t-SNE\")\n",
    "print(f\"📊 Final dataset shapes:\")\n",
    "print(f\"   • Scaled features: {X_scaled.shape}\")\n",
    "print(f\"   • PCA features: {X_pca_df.shape}\")\n",
    "print(f\"   • t-SNE features: {X_tsne_df.shape}\")\n",
    "\n",
    "# Save transformation objects for consistency\n",
    "import pickle\n",
    "with open('../Data/power_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(power_transformer, f)\n",
    "print(\"   • Power transformer saved for future use\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
