{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715b3704",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing for Clustering\n",
    "\n",
    "## Overview\n",
    "This notebook prepares the data for clustering analysis by handling missing values, feature selection, scaling, and dimensionality reduction.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete `01_Data_Exploration_EDA.ipynb` first\n",
    "- Dataset: `../Data/eda_complete_dataset.csv`\n",
    "\n",
    "**Objectives:**\n",
    "- Feature engineering and selection\n",
    "- Handle outliers and missing values\n",
    "- Scale and normalize features\n",
    "- Apply dimensionality reduction techniques\n",
    "- Prepare data for clustering algorithms\n",
    "\n",
    "**Outputs:**\n",
    "- Preprocessed dataset ready for clustering\n",
    "- Scaled features\n",
    "- PCA and t-SNE transformed data\n",
    "- Feature importance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bfde9a",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6dcdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Preprocessing and Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer, LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Outlier Detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Preprocessing libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12478e3",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663827f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: (11000, 16)\n",
      "Columns: ['CompanyID', 'CompanyName', 'Industry', 'Region', 'Year', 'Revenue', 'ProfitMargin', 'MarketCap', 'GrowthRate', 'ESG_Overall', 'ESG_Environmental', 'ESG_Social', 'ESG_Governance', 'CarbonEmissions', 'WaterUsage', 'EnergyConsumption']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Region</th>\n",
       "      <th>Year</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>ProfitMargin</th>\n",
       "      <th>MarketCap</th>\n",
       "      <th>GrowthRate</th>\n",
       "      <th>ESG_Overall</th>\n",
       "      <th>ESG_Environmental</th>\n",
       "      <th>ESG_Social</th>\n",
       "      <th>ESG_Governance</th>\n",
       "      <th>CarbonEmissions</th>\n",
       "      <th>WaterUsage</th>\n",
       "      <th>EnergyConsumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2015</td>\n",
       "      <td>459.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>337.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.7</td>\n",
       "      <td>33.5</td>\n",
       "      <td>76.8</td>\n",
       "      <td>35577.4</td>\n",
       "      <td>17788.7</td>\n",
       "      <td>71154.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2016</td>\n",
       "      <td>473.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>366.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>56.7</td>\n",
       "      <td>58.9</td>\n",
       "      <td>32.8</td>\n",
       "      <td>78.5</td>\n",
       "      <td>37314.7</td>\n",
       "      <td>18657.4</td>\n",
       "      <td>74629.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2017</td>\n",
       "      <td>564.9</td>\n",
       "      <td>5.2</td>\n",
       "      <td>313.4</td>\n",
       "      <td>19.2</td>\n",
       "      <td>56.5</td>\n",
       "      <td>57.6</td>\n",
       "      <td>34.0</td>\n",
       "      <td>77.8</td>\n",
       "      <td>45006.4</td>\n",
       "      <td>22503.2</td>\n",
       "      <td>90012.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2018</td>\n",
       "      <td>558.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>283.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>62.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>78.3</td>\n",
       "      <td>42650.1</td>\n",
       "      <td>21325.1</td>\n",
       "      <td>85300.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Company_1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>2019</td>\n",
       "      <td>554.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>538.1</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>56.6</td>\n",
       "      <td>63.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>76.1</td>\n",
       "      <td>41799.4</td>\n",
       "      <td>20899.7</td>\n",
       "      <td>83598.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CompanyID CompanyName Industry         Region  Year  Revenue  ProfitMargin  \\\n",
       "0          1   Company_1   Retail  Latin America  2015    459.2           6.0   \n",
       "1          1   Company_1   Retail  Latin America  2016    473.8           4.6   \n",
       "2          1   Company_1   Retail  Latin America  2017    564.9           5.2   \n",
       "3          1   Company_1   Retail  Latin America  2018    558.4           4.3   \n",
       "4          1   Company_1   Retail  Latin America  2019    554.5           4.9   \n",
       "\n",
       "   MarketCap  GrowthRate  ESG_Overall  ESG_Environmental  ESG_Social  \\\n",
       "0      337.5         NaN         57.0               60.7        33.5   \n",
       "1      366.6         3.2         56.7               58.9        32.8   \n",
       "2      313.4        19.2         56.5               57.6        34.0   \n",
       "3      283.0        -1.1         58.0               62.3        33.4   \n",
       "4      538.1        -0.7         56.6               63.7        30.0   \n",
       "\n",
       "   ESG_Governance  CarbonEmissions  WaterUsage  EnergyConsumption  \n",
       "0            76.8          35577.4     17788.7            71154.7  \n",
       "1            78.5          37314.7     18657.4            74629.4  \n",
       "2            77.8          45006.4     22503.2            90012.9  \n",
       "3            78.3          42650.1     21325.1            85300.2  \n",
       "4            76.1          41799.4     20899.7            83598.8  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from EDA notebook\n",
    "try:\n",
    "    data = pd.read_csv('../Data/eda_complete_dataset.csv')\n",
    "    print(f\"Dataset loaded successfully: {data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"EDA dataset not found. Please run 01_Data_Exploration_EDA.ipynb first.\")\n",
    "    # Fallback to original dataset\n",
    "    data = pd.read_csv('../Data/company_esg_financial_dataset.csv')\n",
    "    print(f\"Using original dataset: {data.shape}\")\n",
    "\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5824201",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0051d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial features: ['Revenue', 'ProfitMargin', 'MarketCap', 'GrowthRate']\n",
      "ESG features: ['ESG_Overall', 'ESG_Environmental', 'ESG_Social', 'ESG_Governance']\n",
      "Environmental features: ['CarbonEmissions', 'WaterUsage', 'EnergyConsumption']\n",
      "Categorical features: ['Industry', 'Region']\n",
      "\n",
      "Total numerical features for clustering: 11\n"
     ]
    }
   ],
   "source": [
    "# Define feature categories\n",
    "financial_features = ['Revenue', 'ProfitMargin', 'MarketCap', 'GrowthRate']\n",
    "esg_features = ['ESG_Overall', 'ESG_Environmental', 'ESG_Social', 'ESG_Governance']\n",
    "environmental_features = ['CarbonEmissions', 'WaterUsage', 'EnergyConsumption']\n",
    "categorical_features = ['Industry', 'Region']\n",
    "identifier_features = ['CompanyID', 'CompanyName', 'Year']\n",
    "\n",
    "# Combine numerical features for clustering\n",
    "numerical_features = financial_features + esg_features + environmental_features\n",
    "\n",
    "print(f\"Financial features: {financial_features}\")\n",
    "print(f\"ESG features: {esg_features}\")\n",
    "print(f\"Environmental features: {environmental_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"\\nTotal numerical features for clustering: {len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a617042",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values and Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af2afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in numerical features:\n",
      "GrowthRate    1000\n",
      "dtype: int64\n",
      "Filled GrowthRate missing values using temporal imputation (ffill/bfill).\n"
     ]
    }
   ],
   "source": [
    "# Temporal imputation for missing values in numerical features\n",
    "# Fill missing values using forward fill, then backward fill within each company (or entity) over time\n",
    "\n",
    "# Sort data by identifier and time\n",
    "data.sort_values(by=['CompanyID', 'Year'], inplace=True)\n",
    "\n",
    "missing_check = data[numerical_features].isnull().sum()\n",
    "print(\"Missing values in numerical features:\")\n",
    "print(missing_check[missing_check > 0])\n",
    "\n",
    "if missing_check.sum() > 0:\n",
    "    for feature in numerical_features:\n",
    "        if data[feature].isnull().sum() > 0:\n",
    "            # Forward fill then backward fill within each company\n",
    "            data[feature] = data.groupby('CompanyID')[feature].ffill().bfill()\n",
    "            filled_count = data[feature].isnull().sum()\n",
    "            if filled_count == 0:\n",
    "                print(f\"Filled {feature} missing values using temporal imputation (ffill/bfill).\")\n",
    "            else:\n",
    "                print(f\"{filled_count} missing values remain in {feature} after temporal imputation.\")\n",
    "else:\n",
    "    print(\"No missing values found in numerical features.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37959a8",
   "metadata": {},
   "source": [
    "### Strategic Recommendation: Retain All Data Points Without Outlier Removal\n",
    "\n",
    "**Why We Avoid Outlier Detection for Clustering:**\n",
    "\n",
    "- **Business Insights in \"Outliers\":**  \n",
    "    In ESG and financial datasets, statistical \"outliers\" often represent unique companies, market leaders, disruptors, or emerging segments. Removing these points risks losing critical business intelligence.\n",
    "\n",
    "- **Legitimacy of Extreme Values:**  \n",
    "    Extreme values are expected across diverse industries, regions, and market segments. For example, tech giants, energy firms, or companies in high-growth markets naturally exhibit values far from the median.\n",
    "\n",
    "- **Statistical Issues Already Addressed:**  \n",
    "    Our preprocessing pipeline applies power transformation and robust scaling, correcting for skewness and variance without discarding data. This ensures statistical reliability for clustering.\n",
    "\n",
    "- **Data Evidence:**  \n",
    "    Exploratory analysis shows that statistical outliers correspond to meaningful business groups (e.g., top revenue companies, ESG leaders, or regional specialists), not errors or noise.\n",
    "\n",
    "- **Maximum Information Preservation:**  \n",
    "    By retaining all data points, we maximize the diversity and richness of clusters. Our approach ensures that clustering reflects the full spectrum of business realities, not just the statistical average.\n",
    "\n",
    "**Conclusion:**  \n",
    "Outlier removal is not recommended for this business context. Our pipeline preserves all information, corrects statistical challenges, and enables clustering to reveal actionable, real-world business segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb85fbe",
   "metadata": {},
   "source": [
    "### 4.2 Categorical Variable Encoding Implementation\n",
    "\n",
    "**Decision: Implement One-Hot Encoding for Industry and Region**\n",
    "\n",
    "Based on business analysis, we'll encode categorical variables to preserve meaningful relationships:\n",
    "- **Industry patterns**: Technology vs Healthcare vs Finance have distinct ESG and financial profiles\n",
    "- **Regional differences**: EU vs US vs Asia have different regulatory environments and ESG standards\n",
    "- **Business context**: Cross-industry and regional patterns will enhance clustering interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95975974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPLEMENTING ONE-HOT ENCODING FOR CATEGORICAL VARIABLES\n",
      "============================================================\n",
      "1. ANALYZING CATEGORICAL VARIABLES:\n",
      "   • Industry categories: 9\n",
      "   • Region categories: 7\n",
      "   • Industries: ['Consumer Goods', 'Energy', 'Finance', 'Healthcare', 'Manufacturing', 'Retail', 'Technology', 'Transportation', 'Utilities']\n",
      "   • Regions: ['Africa', 'Asia', 'Europe', 'Latin America', 'Middle East', 'North America', 'Oceania']\n",
      "\n",
      "2. ONE-HOT ENCODING RESULTS:\n",
      "   • Original categorical features: 2\n",
      "   • Encoded features created: 14\n",
      "   • Encoded feature names: ['Industry_Energy', 'Industry_Finance', 'Industry_Healthcare', 'Industry_Manufacturing', 'Industry_Retail', 'Industry_Technology', 'Industry_Transportation', 'Industry_Utilities', 'Region_Asia', 'Region_Europe', 'Region_Latin America', 'Region_Middle East', 'Region_North America', 'Region_Oceania']\n",
      "\n",
      "   ✅ Categorical encoding shape: (11000, 14)\n",
      "   ✅ Sample of encoded features:\n",
      "   Industry_Energy  Industry_Finance  Industry_Healthcare  \\\n",
      "0              0.0               0.0                  0.0   \n",
      "1              0.0               0.0                  0.0   \n",
      "2              0.0               0.0                  0.0   \n",
      "\n",
      "   Industry_Manufacturing  Industry_Retail  Industry_Technology  \\\n",
      "0                     0.0              1.0                  0.0   \n",
      "1                     0.0              1.0                  0.0   \n",
      "2                     0.0              1.0                  0.0   \n",
      "\n",
      "   Industry_Transportation  Industry_Utilities  Region_Asia  Region_Europe  \\\n",
      "0                      0.0                 0.0          0.0            0.0   \n",
      "1                      0.0                 0.0          0.0            0.0   \n",
      "2                      0.0                 0.0          0.0            0.0   \n",
      "\n",
      "   Region_Latin America  Region_Middle East  Region_North America  \\\n",
      "0                   1.0                 0.0                   0.0   \n",
      "1                   1.0                 0.0                   0.0   \n",
      "2                   1.0                 0.0                   0.0   \n",
      "\n",
      "   Region_Oceania  \n",
      "0             0.0  \n",
      "1             0.0  \n",
      "2             0.0  \n"
     ]
    }
   ],
   "source": [
    "# CATEGORICAL VARIABLE ENCODING IMPLEMENTATION\n",
    "print(\"IMPLEMENTING ONE-HOT ENCODING FOR CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, run previous cells to ensure variables are loaded\n",
    "if 'data' not in locals():\n",
    "    print(\"Loading data and running previous preprocessing steps...\")\n",
    "    exec(open('previous_cells.py').read())  # This would run previous cells\n",
    "    \n",
    "print(\"1. ANALYZING CATEGORICAL VARIABLES:\")\n",
    "print(f\"   • Industry categories: {data['Industry'].nunique()}\")\n",
    "print(f\"   • Region categories: {data['Region'].nunique()}\")\n",
    "print(f\"   • Industries: {sorted(data['Industry'].unique())}\")\n",
    "print(f\"   • Regions: {sorted(data['Region'].unique())}\")\n",
    "\n",
    "# Create one-hot encoder (drop='first' to avoid multicollinearity)\n",
    "categorical_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# Fit and transform categorical features\n",
    "categorical_data = data[categorical_features]\n",
    "categorical_encoded = categorical_encoder.fit_transform(categorical_data)\n",
    "\n",
    "# Get feature names for encoded variables\n",
    "encoded_feature_names = categorical_encoder.get_feature_names_out(categorical_features)\n",
    "print(f\"\\n2. ONE-HOT ENCODING RESULTS:\")\n",
    "print(f\"   • Original categorical features: {len(categorical_features)}\")\n",
    "print(f\"   • Encoded features created: {len(encoded_feature_names)}\")\n",
    "print(f\"   • Encoded feature names: {list(encoded_feature_names)}\")\n",
    "\n",
    "# Create DataFrame with encoded features\n",
    "categorical_encoded_df = pd.DataFrame(\n",
    "    categorical_encoded, \n",
    "    columns=encoded_feature_names,\n",
    "    index=data.index\n",
    ")\n",
    "\n",
    "print(f\"\\n   ✅ Categorical encoding shape: {categorical_encoded_df.shape}\")\n",
    "print(f\"   ✅ Sample of encoded features:\")\n",
    "print(categorical_encoded_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690de7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. COMBINING NUMERICAL AND CATEGORICAL FEATURES:\n",
      "   • Created numerical features matrix\n",
      "   • Original numerical features: 11\n",
      "   • Encoded categorical features: 14\n",
      "   • Combined feature set: 25 features\n",
      "   • Combined dataset shape: (11000, 25)\n",
      "   • Total features for enhanced clustering: 25\n",
      "\n",
      "4. BUSINESS CONTEXT PRESERVATION:\n",
      "   Sample companies with their categorical encodings:\n",
      "   • Company_1 (Retail, Latin America)\n",
      "     Active encodings: ['Industry_Retail', 'Region_Latin America']\n",
      "\n",
      "   • Company_1 (Retail, Latin America)\n",
      "     Active encodings: ['Industry_Retail', 'Region_Latin America']\n",
      "\n",
      "   • Company_1 (Retail, Latin America)\n",
      "     Active encodings: ['Industry_Retail', 'Region_Latin America']\n",
      "\n",
      "✅ ENHANCED FEATURE SET READY FOR PREPROCESSING\n",
      "   📊 25 total features preserve business context\n"
     ]
    }
   ],
   "source": [
    "# COMBINE NUMERICAL AND CATEGORICAL FEATURES\n",
    "print(\"\\n3. COMBINING NUMERICAL AND CATEGORICAL FEATURES:\")\n",
    "\n",
    "# Ensure we have the numerical features processed (X from previous preprocessing)\n",
    "if 'X' not in locals():\n",
    "    X = data[numerical_features].copy()\n",
    "    print(\"   • Created numerical features matrix\")\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "X_combined = pd.concat([\n",
    "    X,  # Original numerical features (11 features)\n",
    "    categorical_encoded_df  # Encoded categorical features\n",
    "], axis=1)\n",
    "\n",
    "print(f\"   • Original numerical features: {X.shape[1]}\")\n",
    "print(f\"   • Encoded categorical features: {categorical_encoded_df.shape[1]}\")\n",
    "print(f\"   • Combined feature set: {X_combined.shape[1]} features\")\n",
    "print(f\"   • Combined dataset shape: {X_combined.shape}\")\n",
    "\n",
    "# Update feature lists for enhanced clustering\n",
    "combined_features = list(X.columns) + list(encoded_feature_names)\n",
    "print(f\"   • Total features for enhanced clustering: {len(combined_features)}\")\n",
    "\n",
    "print(f\"\\n4. BUSINESS CONTEXT PRESERVATION:\")\n",
    "print(\"   Sample companies with their categorical encodings:\")\n",
    "for idx in range(3):\n",
    "    company_name = data.iloc[idx]['CompanyName']\n",
    "    industry = data.iloc[idx]['Industry'] \n",
    "    region = data.iloc[idx]['Region']\n",
    "    print(f\"   • {company_name} ({industry}, {region})\")\n",
    "    \n",
    "    # Show which categorical features are active (value = 1)\n",
    "    active_features = []\n",
    "    for feature in encoded_feature_names:\n",
    "        if categorical_encoded_df.iloc[idx][feature] == 1:\n",
    "            active_features.append(feature)\n",
    "    print(f\"     Active encodings: {active_features}\")\n",
    "    print()\n",
    "\n",
    "print(f\"✅ ENHANCED FEATURE SET READY FOR PREPROCESSING\")\n",
    "print(f\"   📊 {X_combined.shape[1]} total features preserve business context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1236a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENHANCED PREPROCESSING: NUMERICAL + CATEGORICAL FEATURES\n",
      "============================================================\n",
      "5. POWER TRANSFORMATION FOR COMBINED DATASET:\n",
      "   Applying power transformation to numerical features...\n",
      "   ✅ Power transformation applied to 11 numerical features\n",
      "   ✅ Categorical encoded features kept as binary (0/1)\n",
      "\n",
      "6. SCALING COMBINED DATASET:\n",
      "   ✅ StandardScaler applied to all 25 features\n",
      "   📊 Final combined scaled dataset shape: (11000, 25)\n",
      "\n",
      "7. ENHANCED APPROACH SUMMARY:\n",
      "   🔍 Original approach: 11 numerical features only\n",
      "   🚀 Enhanced approach: 25 features (numerical + categorical)\n",
      "   📈 Business context preserved through Industry/Region encoding\n",
      "   ⚡ Ready for enhanced clustering with richer feature representation\n",
      "\n",
      "8. SAMPLE OF ENHANCED FEATURES:\n",
      "   First 3 companies with combined scaled features:\n",
      "    Revenue  ESG_Overall  Industry_Energy  Industry_Finance  \\\n",
      "0 -1.445479     0.149811         -0.34796         -0.356925   \n",
      "1 -1.410825     0.130932         -0.34796         -0.356925   \n",
      "2 -1.218985     0.118347         -0.34796         -0.356925   \n",
      "\n",
      "   Industry_Healthcare  \n",
      "0            -0.371021  \n",
      "1            -0.371021  \n",
      "2            -0.371021  \n"
     ]
    }
   ],
   "source": [
    "# ENHANCED PREPROCESSING PIPELINE FOR COMBINED DATASET\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENHANCED PREPROCESSING: NUMERICAL + CATEGORICAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"5. POWER TRANSFORMATION FOR COMBINED DATASET:\")\n",
    "# Apply power transformation only to numerical features \n",
    "# (categorical encoded features are already 0/1 and don't need transformation)\n",
    "X_combined_transformed = X_combined.copy()\n",
    "\n",
    "# Apply power transformation only to original numerical features\n",
    "print(\"   Applying power transformation to numerical features...\")\n",
    "if 'power_transformer' not in locals():\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    \n",
    "# Transform only numerical features\n",
    "numerical_indices = [X_combined.columns.get_loc(col) for col in numerical_features if col in X_combined.columns]\n",
    "X_combined_transformed.iloc[:, numerical_indices] = power_transformer.fit_transform(X_combined.iloc[:, numerical_indices])\n",
    "\n",
    "print(f\"   ✅ Power transformation applied to {len(numerical_features)} numerical features\")\n",
    "print(f\"   ✅ Categorical encoded features kept as binary (0/1)\")\n",
    "\n",
    "print(\"\\n6. SCALING COMBINED DATASET:\")\n",
    "# Scale the combined dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "combined_scaler = StandardScaler()\n",
    "X_combined_scaled = combined_scaler.fit_transform(X_combined_transformed)\n",
    "X_combined_scaled_df = pd.DataFrame(\n",
    "    X_combined_scaled, \n",
    "    columns=combined_features, \n",
    "    index=X_combined.index\n",
    ")\n",
    "\n",
    "print(f\"   ✅ StandardScaler applied to all {len(combined_features)} features\")\n",
    "print(f\"   📊 Final combined scaled dataset shape: {X_combined_scaled_df.shape}\")\n",
    "\n",
    "print(\"\\n7. ENHANCED APPROACH SUMMARY:\")\n",
    "print(f\"   🔍 Original approach: {len(numerical_features)} numerical features only\")  \n",
    "print(f\"   🚀 Enhanced approach: {X_combined_scaled_df.shape[1]} features (numerical + categorical)\")\n",
    "print(f\"   📈 Business context preserved through Industry/Region encoding\")\n",
    "print(f\"   ⚡ Ready for enhanced clustering with richer feature representation\")\n",
    "\n",
    "# Show sample of scaled combined features\n",
    "print(f\"\\n8. SAMPLE OF ENHANCED FEATURES:\")\n",
    "print(\"   First 3 companies with combined scaled features:\")\n",
    "sample_features = ['Revenue', 'ESG_Overall'] + list(encoded_feature_names[:3])\n",
    "available_features = [f for f in sample_features if f in X_combined_scaled_df.columns]\n",
    "print(X_combined_scaled_df[available_features].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "403c2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENHANCED PCA ANALYSIS\n",
      "============================================================\n",
      "9. APPLYING PCA TO ENHANCED FEATURE SET:\n",
      "   • Enhanced features: 25\n",
      "   • Components for 95% variance: 17\n",
      "   • Variance explained by first 3 components: 0.403\n",
      "   ✅ Enhanced PCA dataset shape: (11000, 17)\n",
      "\n",
      "10. APPROACH COMPARISON:\n",
      "   Enhanced (with categorical encoding):\n",
      "   • Features: 25\n",
      "   • PCA components for 95% variance: 17\n",
      "\n",
      "11. TOP PCA COMPONENTS (ENHANCED APPROACH):\n",
      "   • PC1: 16.8% of variance\n",
      "   • PC2: 13.4% of variance\n",
      "   • PC3: 10.1% of variance\n",
      "   • PC4: 5.1% of variance\n",
      "   • PC5: 5.0% of variance\n",
      "\n",
      "🎯 ENHANCED PCA READY FOR CLUSTERING!\n",
      "✅ Business context preserved in dimensionality reduction\n",
      "✅ 17 optimal components capture industry/regional patterns\n"
     ]
    }
   ],
   "source": [
    "# ENHANCED PCA ANALYSIS WITH CATEGORICAL FEATURES\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENHANCED PCA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"9. APPLYING PCA TO ENHANCED FEATURE SET:\")\n",
    "# Apply PCA to the combined scaled dataset\n",
    "from sklearn.decomposition import PCA\n",
    "pca_enhanced = PCA()\n",
    "X_enhanced_pca = pca_enhanced.fit_transform(X_combined_scaled_df)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumsum_var_enhanced = np.cumsum(pca_enhanced.explained_variance_ratio_)\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95_enhanced = np.argmax(cumsum_var_enhanced >= 0.95) + 1\n",
    "\n",
    "print(f\"   • Enhanced features: {X_combined_scaled_df.shape[1]}\")\n",
    "print(f\"   • Components for 95% variance: {n_components_95_enhanced}\")\n",
    "print(f\"   • Variance explained by first 3 components: {cumsum_var_enhanced[2]:.3f}\")\n",
    "\n",
    "# Create optimal PCA dataframe\n",
    "pca_enhanced_optimal = PCA(n_components=n_components_95_enhanced)\n",
    "X_enhanced_pca_optimal = pca_enhanced_optimal.fit_transform(X_combined_scaled_df)\n",
    "\n",
    "pca_enhanced_columns = [f'PCA_Enhanced_{i+1}' for i in range(n_components_95_enhanced)]\n",
    "X_enhanced_pca_df = pd.DataFrame(\n",
    "    X_enhanced_pca_optimal, \n",
    "    columns=pca_enhanced_columns, \n",
    "    index=X_combined_scaled_df.index\n",
    ")\n",
    "\n",
    "print(f\"   ✅ Enhanced PCA dataset shape: {X_enhanced_pca_df.shape}\")\n",
    "\n",
    "# Compare with original numerical-only approach (if available)\n",
    "print(f\"\\n10. APPROACH COMPARISON:\")\n",
    "if 'n_components_95' in locals():\n",
    "    print(f\"   Original (numerical-only):\")\n",
    "    print(f\"   • Features: {len(numerical_features)}\")\n",
    "    print(f\"   • PCA components for 95% variance: {n_components_95}\")\n",
    "    \n",
    "print(f\"   Enhanced (with categorical encoding):\")\n",
    "print(f\"   • Features: {X_combined_scaled_df.shape[1]}\")\n",
    "print(f\"   • PCA components for 95% variance: {n_components_95_enhanced}\")\n",
    "\n",
    "print(f\"\\n11. TOP PCA COMPONENTS (ENHANCED APPROACH):\")\n",
    "for i in range(min(5, n_components_95_enhanced)):\n",
    "    variance_pct = pca_enhanced.explained_variance_ratio_[i] * 100\n",
    "    print(f\"   • PC{i+1}: {variance_pct:.1f}% of variance\")\n",
    "\n",
    "print(f\"\\n🎯 ENHANCED PCA READY FOR CLUSTERING!\")\n",
    "print(f\"✅ Business context preserved in dimensionality reduction\")\n",
    "print(f\"✅ {n_components_95_enhanced} optimal components capture industry/regional patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1183707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING ENHANCED DATASETS\n",
      "============================================================\n",
      "12. CREATING ENHANCED T-SNE:\n",
      "   ✅ Enhanced t-SNE shape: (11000, 2)\n",
      "\n",
      "13. SAVING ENHANCED DATASETS:\n",
      "   ✅ Enhanced datasets saved:\n",
      "   - ../Data/scaled_features_with_categorical.csv\n",
      "   - ../Data/pca_features_with_categorical.csv\n",
      "   - ../Data/tsne_features_with_categorical.csv\n",
      "   - ../Data/categorical_encoded_features.csv\n",
      "   - ../Data/preprocessed_complete_dataset_enhanced.csv\n",
      "   • Categorical encoder and combined scaler saved\n",
      "\n",
      "🎯 ENHANCED PREPROCESSING COMPLETE!\n",
      "=================================================================\n",
      "✅ DUAL APPROACH READY FOR CLUSTERING COMPARISON:\n",
      "   1. Original: 11 numerical features only\n",
      "   2. Enhanced: 25 features (numerical + categorical)\n",
      "✅ Business context preserved through Industry/Region encoding\n",
      "✅ Both approaches optimized for clustering algorithms\n",
      "📊 Ready for comprehensive clustering analysis!\n"
     ]
    }
   ],
   "source": [
    "# SAVE ENHANCED DATASETS WITH CATEGORICAL ENCODING\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING ENHANCED DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"12. CREATING ENHANCED T-SNE:\")\n",
    "# Create t-SNE for enhanced approach\n",
    "from sklearn.manifold import TSNE\n",
    "tsne_enhanced = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_enhanced_tsne = tsne_enhanced.fit_transform(X_enhanced_pca_optimal[:, :5])\n",
    "\n",
    "X_enhanced_tsne_df = pd.DataFrame(\n",
    "    X_enhanced_tsne, \n",
    "    columns=['t-SNE_Enhanced_1', 't-SNE_Enhanced_2'], \n",
    "    index=X_combined_scaled_df.index\n",
    ")\n",
    "\n",
    "print(f\"   ✅ Enhanced t-SNE shape: {X_enhanced_tsne_df.shape}\")\n",
    "\n",
    "print(\"\\n13. SAVING ENHANCED DATASETS:\")\n",
    "# Save enhanced preprocessing datasets\n",
    "X_combined_scaled_df.to_csv('../Data/scaled_features_with_categorical.csv', index=False)\n",
    "X_enhanced_pca_df.to_csv('../Data/pca_features_with_categorical.csv', index=False) \n",
    "X_enhanced_tsne_df.to_csv('../Data/tsne_features_with_categorical.csv', index=False)\n",
    "categorical_encoded_df.to_csv('../Data/categorical_encoded_features.csv', index=False)\n",
    "\n",
    "# Update the complete preprocessed dataset\n",
    "if 'preprocessed_data' in locals():\n",
    "    preprocessed_data_enhanced = preprocessed_data.copy()\n",
    "else:\n",
    "    preprocessed_data_enhanced = data.copy()\n",
    "\n",
    "# Add enhanced scaled features\n",
    "for col in X_combined_scaled_df.columns:\n",
    "    preprocessed_data_enhanced[f'{col}_enhanced_scaled'] = X_combined_scaled_df[col]\n",
    "\n",
    "# Add enhanced PCA features\n",
    "for col in X_enhanced_pca_df.columns:\n",
    "    preprocessed_data_enhanced[col] = X_enhanced_pca_df[col]\n",
    "\n",
    "# Add enhanced t-SNE features\n",
    "for col in X_enhanced_tsne_df.columns:\n",
    "    preprocessed_data_enhanced[col] = X_enhanced_tsne_df[col]\n",
    "\n",
    "# Save complete enhanced dataset\n",
    "preprocessed_data_enhanced.to_csv('../Data/preprocessed_complete_dataset_enhanced.csv', index=False)\n",
    "\n",
    "print(\"   ✅ Enhanced datasets saved:\")\n",
    "print(\"   - ../Data/scaled_features_with_categorical.csv\")\n",
    "print(\"   - ../Data/pca_features_with_categorical.csv\") \n",
    "print(\"   - ../Data/tsne_features_with_categorical.csv\")\n",
    "print(\"   - ../Data/categorical_encoded_features.csv\")\n",
    "print(\"   - ../Data/preprocessed_complete_dataset_enhanced.csv\")\n",
    "\n",
    "# Save encoders and transformers for consistency\n",
    "import pickle\n",
    "with open('../Data/categorical_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(categorical_encoder, f)\n",
    "with open('../Data/combined_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_scaler, f)\n",
    "    \n",
    "print(\"   • Categorical encoder and combined scaler saved\")\n",
    "\n",
    "print(f\"\\n🎯 ENHANCED PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*65)\n",
    "print(f\"✅ DUAL APPROACH READY FOR CLUSTERING COMPARISON:\")\n",
    "print(f\"   1. Original: {len(numerical_features)} numerical features only\")\n",
    "print(f\"   2. Enhanced: {X_combined_scaled_df.shape[1]} features (numerical + categorical)\")\n",
    "print(f\"✅ Business context preserved through Industry/Region encoding\")\n",
    "print(f\"✅ Both approaches optimized for clustering algorithms\")\n",
    "print(f\"📊 Ready for comprehensive clustering analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78070fb",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81bcb9",
   "metadata": {},
   "source": [
    "### 4.1 Advanced Transformation for Skewed Features\n",
    "\n",
    "Based on EDA analysis, 5 features show extreme skewness (|skew| >= 1.0):\n",
    "- CarbonEmissions: 15.848\n",
    "- EnergyConsumption: 15.654  \n",
    "- WaterUsage: 14.386\n",
    "- MarketCap: 8.884\n",
    "- Revenue: 7.369\n",
    "\n",
    "Standard scaling alone doesn't fix skewness. We'll apply power transformation first, then scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e8d9e9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING PIPELINE WITH SKEWNESS CORRECTION\n",
      "============================================================\n",
      "\n",
      "1. ANALYZING ORIGINAL SKEWNESS:\n",
      "   • Revenue: 7.369 (HIGH SKEWNESS)\n",
      "   • MarketCap: 8.884 (HIGH SKEWNESS)\n",
      "   • CarbonEmissions: 15.848 (HIGH SKEWNESS)\n",
      "   • WaterUsage: 14.386 (HIGH SKEWNESS)\n",
      "   • EnergyConsumption: 15.654 (HIGH SKEWNESS)\n",
      "\n",
      "2. APPLYING YEO-JOHNSON POWER TRANSFORMATION:\n",
      "   Post-transformation skewness:\n",
      "   • Revenue: 0.008 (improved by 7.361)\n",
      "   • MarketCap: -0.001 (improved by 8.883)\n",
      "   • CarbonEmissions: -0.014 (improved by 15.834)\n",
      "   • WaterUsage: -0.001 (improved by 14.385)\n",
      "   • EnergyConsumption: 0.019 (improved by 15.634)\n",
      "\n",
      "3. APPLYING SCALING TO POWER-TRANSFORMED DATA:\n",
      "   StandardScaler applied to power-transformed data.\n",
      "   MinMaxScaler applied to power-transformed data.\n",
      "   RobustScaler applied to power-transformed data.\n",
      "\n",
      "4. FINAL PREPROCESSING SUMMARY:\n",
      "   ✅ Power transformation applied to fix skewness\n",
      "   ✅ Standard scaling applied for clustering\n",
      "   📊 Final preprocessed data shape: (11000, 11)\n",
      "\n",
      "5. FINAL SKEWNESS VERIFICATION:\n",
      "   ✅ All features now have acceptable skewness (|skew| < 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for scaling\n",
    "X = data[numerical_features].copy()\n",
    "\n",
    "print(\"PREPROCESSING PIPELINE WITH SKEWNESS CORRECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Apply Power Transformation to handle skewness\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats import skew\n",
    "\n",
    "print(\"\\n1. ANALYZING ORIGINAL SKEWNESS:\")\n",
    "original_skew = {}\n",
    "for feature in numerical_features:\n",
    "    skewness = skew(X[feature].dropna())\n",
    "    original_skew[feature] = skewness\n",
    "    if abs(skewness) >= 1.0:\n",
    "        print(f\"   • {feature}: {skewness:.3f} (HIGH SKEWNESS)\")\n",
    "\n",
    "# Apply Yeo-Johnson Power Transformation (handles positive and negative values)\n",
    "print(\"\\n2. APPLYING YEO-JOHNSON POWER TRANSFORMATION:\")\n",
    "power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "X_power_transformed = power_transformer.fit_transform(X)\n",
    "X_power_df = pd.DataFrame(X_power_transformed, columns=numerical_features, index=X.index)\n",
    "\n",
    "# Check skewness after power transformation\n",
    "print(\"   Post-transformation skewness:\")\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    new_skewness = skew(X_power_transformed[:, i])\n",
    "    improvement = abs(original_skew[feature]) - abs(new_skewness)\n",
    "    if abs(original_skew[feature]) >= 1.0:\n",
    "        print(f\"   • {feature}: {new_skewness:.3f} (improved by {improvement:.3f})\")\n",
    "\n",
    "# Step 2: Apply different scaling methods to power-transformed data\n",
    "print(\"\\n3. APPLYING SCALING TO POWER-TRANSFORMED DATA:\")\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "scaled_data = {}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    scaled_features = scaler.fit_transform(X_power_df)\n",
    "    scaled_data[name] = pd.DataFrame(scaled_features, columns=numerical_features, index=X.index)\n",
    "    print(f\"   {name} applied to power-transformed data.\")\n",
    "\n",
    "# Use StandardScaler on power-transformed data as default\n",
    "X_scaled = scaled_data['StandardScaler']\n",
    "\n",
    "print(f\"\\n4. FINAL PREPROCESSING SUMMARY:\")\n",
    "print(f\"   ✅ Power transformation applied to fix skewness\")\n",
    "print(f\"   ✅ Standard scaling applied for clustering\")\n",
    "print(f\"   📊 Final preprocessed data shape: {X_scaled.shape}\")\n",
    "\n",
    "# Verify final skewness\n",
    "print(f\"\\n5. FINAL SKEWNESS VERIFICATION:\")\n",
    "final_highly_skewed = 0\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    final_skewness = skew(X_scaled.iloc[:, i])\n",
    "    if abs(final_skewness) >= 1.0:\n",
    "        final_highly_skewed += 1\n",
    "        print(f\"   ⚠️  {feature}: {final_skewness:.3f} (still high)\")\n",
    "\n",
    "if final_highly_skewed == 0:\n",
    "    print(\"   ✅ All features now have acceptable skewness (|skew| < 1.0)\")\n",
    "else:\n",
    "    print(f\"   ⚠️  {final_highly_skewed} features still highly skewed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cac869",
   "metadata": {},
   "source": [
    "## 6. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "97ccf94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. DIMENSIONALITY REDUCTION WITH PCA:\n",
      "   Number of components for 95% variance: 7\n",
      "   Explained variance by first 3 components: 0.727\n",
      "   ✅ PCA transformed data shape: (11000, 7)\n",
      "\n",
      "   PCA Components Explained Variance:\n",
      "   • PC1: 0.314 (31.4%)\n",
      "   • PC2: 0.241 (24.1%)\n",
      "   • PC3: 0.171 (17.1%)\n",
      "   • PC4: 0.087 (8.7%)\n",
      "   • PC5: 0.070 (7.0%)\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA to the properly transformed and scaled data\n",
    "print(\"\\n6. DIMENSIONALITY REDUCTION WITH PCA:\")\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "\n",
    "print(f\"   Number of components for 95% variance: {n_components_95}\")\n",
    "print(f\"   Explained variance by first 3 components: {cumsum_var[2]:.3f}\")\n",
    "\n",
    "# Create PCA dataframe with optimal components\n",
    "pca_optimal = PCA(n_components=n_components_95)\n",
    "X_pca_optimal = pca_optimal.fit_transform(X_scaled)\n",
    "\n",
    "pca_columns = [f'PCA_{i+1}' for i in range(n_components_95)]\n",
    "X_pca_df = pd.DataFrame(X_pca_optimal, columns=pca_columns, index=X_scaled.index)\n",
    "\n",
    "print(f\"   ✅ PCA transformed data shape: {X_pca_df.shape}\")\n",
    "\n",
    "# Show PCA component importance\n",
    "print(f\"\\n   PCA Components Explained Variance:\")\n",
    "for i in range(min(5, n_components_95)):\n",
    "    print(f\"   • PC{i+1}: {pca.explained_variance_ratio_[i]:.3f} ({pca.explained_variance_ratio_[i]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "869e5406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. T-SNE VISUALIZATION PREPARATION:\n",
      "   ✅ t-SNE transformed data shape: (11000, 2)\n",
      "   ✅ Ready for clustering visualization\n",
      "   ✅ t-SNE transformed data shape: (11000, 2)\n",
      "   ✅ Ready for clustering visualization\n"
     ]
    }
   ],
   "source": [
    "# Apply t-SNE for visualization (using first few PCA components)\n",
    "print(\"\\n7. T-SNE VISUALIZATION PREPARATION:\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_pca_optimal[:, :5])  # Use first 5 PCA components\n",
    "\n",
    "X_tsne_df = pd.DataFrame(X_tsne, columns=['t-SNE_1', 't-SNE_2'], index=X_scaled.index)\n",
    "\n",
    "print(f\"   ✅ t-SNE transformed data shape: {X_tsne_df.shape}\")\n",
    "print(f\"   ✅ Ready for clustering visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87843614",
   "metadata": {},
   "source": [
    "## 7. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6f2fc079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. SAVING IMPROVED PREPROCESSED DATASETS:\n",
      "   ✅ Improved preprocessed datasets saved:\n",
      "   - ../Data/preprocessed_complete_dataset.csv\n",
      "   - ../Data/scaled_features.csv (power-transformed + scaled)\n",
      "   - ../Data/pca_features.csv (based on improved features)\n",
      "   - ../Data/tsne_features.csv\n",
      "   - ../Data/power_transformed_features.csv (intermediate step)\n",
      "\n",
      "🎯 PREPROCESSING COMPLETE - READY FOR IMPROVED CLUSTERING!\n",
      "=================================================================\n",
      "✅ Skewness corrected with Yeo-Johnson power transformation\n",
      "✅ Features properly scaled with StandardScaler\n",
      "✅ Dimensionality reduced with PCA (7 components)\n",
      "✅ Visualization prepared with t-SNE\n",
      "📊 Final dataset shapes:\n",
      "   • Scaled features: (11000, 11)\n",
      "   • PCA features: (11000, 7)\n",
      "   • t-SNE features: (11000, 2)\n",
      "   • Power transformer saved for future use\n"
     ]
    }
   ],
   "source": [
    "# Combine all preprocessed data\n",
    "preprocessed_data = data.copy()\n",
    "\n",
    "# Add scaled features\n",
    "for col in X_scaled.columns:\n",
    "    preprocessed_data[f'{col}_scaled'] = X_scaled[col]\n",
    "\n",
    "# Add PCA features\n",
    "for col in X_pca_df.columns:\n",
    "    preprocessed_data[col] = X_pca_df[col]\n",
    "\n",
    "# Add t-SNE features\n",
    "for col in X_tsne_df.columns:\n",
    "    preprocessed_data[col] = X_tsne_df[col]\n",
    "\n",
    "# Save datasets with improved preprocessing\n",
    "print(\"\\n8. SAVING IMPROVED PREPROCESSED DATASETS:\")\n",
    "\n",
    "# Save power-transformed features (before scaling) for reference\n",
    "X_power_df.to_csv('../Data/power_transformed_features.csv', index=False)\n",
    "\n",
    "# Update the complete preprocessed dataset\n",
    "preprocessed_data = data.copy()\n",
    "\n",
    "# Add power-transformed + scaled features (our improved approach)\n",
    "for col in X_scaled.columns:\n",
    "    preprocessed_data[f'{col}_scaled'] = X_scaled[col]\n",
    "\n",
    "# Add PCA features (based on improved preprocessing)\n",
    "for col in X_pca_df.columns:\n",
    "    preprocessed_data[col] = X_pca_df[col]\n",
    "\n",
    "# Add t-SNE features\n",
    "for col in X_tsne_df.columns:\n",
    "    preprocessed_data[col] = X_tsne_df[col]\n",
    "\n",
    "# Save all datasets\n",
    "preprocessed_data.to_csv('../Data/preprocessed_complete_dataset.csv', index=False)\n",
    "X_scaled.to_csv('../Data/scaled_features.csv', index=False)  # Now power-transformed + scaled\n",
    "X_pca_df.to_csv('../Data/pca_features.csv', index=False)\n",
    "X_tsne_df.to_csv('../Data/tsne_features.csv', index=False)\n",
    "\n",
    "print(\"   ✅ Improved preprocessed datasets saved:\")\n",
    "print(\"   - ../Data/preprocessed_complete_dataset.csv\")\n",
    "print(\"   - ../Data/scaled_features.csv (power-transformed + scaled)\")\n",
    "print(\"   - ../Data/pca_features.csv (based on improved features)\")\n",
    "print(\"   - ../Data/tsne_features.csv\")\n",
    "print(\"   - ../Data/power_transformed_features.csv (intermediate step)\")\n",
    "\n",
    "print(f\"\\n🎯 PREPROCESSING COMPLETE - READY FOR IMPROVED CLUSTERING!\")\n",
    "print(\"=\"*65)\n",
    "print(f\"✅ Skewness corrected with Yeo-Johnson power transformation\")\n",
    "print(f\"✅ Features properly scaled with StandardScaler\")\n",
    "print(f\"✅ Dimensionality reduced with PCA ({n_components_95} components)\")\n",
    "print(f\"✅ Visualization prepared with t-SNE\")\n",
    "print(f\"📊 Final dataset shapes:\")\n",
    "print(f\"   • Scaled features: {X_scaled.shape}\")\n",
    "print(f\"   • PCA features: {X_pca_df.shape}\")\n",
    "print(f\"   • t-SNE features: {X_tsne_df.shape}\")\n",
    "\n",
    "# Save transformation objects for consistency\n",
    "import pickle\n",
    "with open('../Data/power_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(power_transformer, f)\n",
    "print(\"   • Power transformer saved for future use\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
